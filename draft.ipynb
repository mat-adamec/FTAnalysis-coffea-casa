{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import uproot\n",
    "%matplotlib inline\n",
    "from coffea import hist\n",
    "#from coffea.analysis_objects import JaggedCandidateArray\n",
    "import coffea.processor as processor\n",
    "from coffea.nanoevents import NanoEventsFactory, NanoAODSchema\n",
    "import awkward as ak\n",
    "import numpy as np\n",
    "import cloudpickle\n",
    "import gzip\n",
    "import time\n",
    "import os\n",
    "import glob\n",
    "from yahist import Hist1D, Hist2D       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /opt/conda/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "Requirement already satisfied: yahist in /opt/conda/lib/python3.8/site-packages (1.11.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.8/site-packages (from yahist) (1.20.2)\n",
      "Requirement already satisfied: plotly in /opt/conda/lib/python3.8/site-packages (from yahist) (4.14.3)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.8/site-packages (from yahist) (1.5.3)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.8/site-packages (from yahist) (3.4.1)\n",
      "Requirement already satisfied: iminuit>=2 in /opt/conda/lib/python3.8/site-packages (from yahist) (2.6.0)\n",
      "Requirement already satisfied: boost-histogram>=1.0 in /opt/conda/lib/python3.8/site-packages (from yahist) (1.0.1)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from plotly->yahist) (1.15.0)\n",
      "Requirement already satisfied: retrying>=1.3.3 in /opt/conda/lib/python3.8/site-packages (from plotly->yahist) (1.3.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib->yahist) (8.1.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib->yahist) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib->yahist) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.8/site-packages (from matplotlib->yahist) (2.8.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.8/site-packages (from matplotlib->yahist) (0.10.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install yahist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Processor(processor.ProcessorABC):\n",
    "    def __init__(self):\n",
    "        dataset_axis = hist.Cat(\"dataset\", \"\") \n",
    "        pt_axis = hist.Bin(\"pt\", \" [GeV]\", [10., 15., 20., 25., 35., 50., 70., 90.])\n",
    "        eta_mu_axis= hist.Bin(\"eta\", \"\", [0., 1.2, 2.1, 2.4])\n",
    "        eta_el_axis= hist.Bin(\"eta\", \"\", [0., 0.8, 1.479, 2.5])\n",
    "        \n",
    "        self._accumulator = processor.dict_accumulator({\n",
    "            'Muon': hist.Hist(\"Counts\", dataset_axis, pt_axis, eta_mu_axis),\n",
    "            'Tight_Muon': hist.Hist(\"Counts\", dataset_axis, pt_axis, eta_mu_axis),\n",
    "            'Electron': hist.Hist(\"Counts\", dataset_axis, pt_axis, eta_el_axis),\n",
    "            'Tight_Electron': hist.Hist(\"Counts\", dataset_axis, pt_axis, eta_el_axis)\n",
    "\n",
    "        })\n",
    "    \n",
    "    @property\n",
    "    def accumulator(self):\n",
    "        \n",
    "        return self._accumulator\n",
    "    \n",
    "    def process(self, events):\n",
    "        output = self.accumulator.identity()\n",
    "        \n",
    "        dataset = events.metadata[\"dataset\"]\n",
    "        \n",
    "        # Skim Muon, Electron, and Jet\n",
    "        Muon = events.Muon\n",
    "        Muon = Muon[( Muon.pt >=10 ) &\n",
    "                           (np.abs(Muon.eta) <= 2.4) &\n",
    "                           (np.abs(Muon.dxy) <= 0.05) &\n",
    "                           (np.abs(Muon.dz) <= 0.1) &\n",
    "                           (np.abs(Muon.sip3d) < 4) &\n",
    "                           (Muon.looseId==1) &\n",
    "                           (Muon.ptErr/Muon.pt < 0.2) &\n",
    "                           (Muon.mediumId==1) ]\n",
    "        \n",
    "        Electron = events.Electron\n",
    "        Electron = Electron[( Electron.pt >10 ) &  #  (Electron.isTriggerSafeNolso) \n",
    "                           (np.abs(Electron.eta+Electron.deltaEtaSC ) < 2.4) &\n",
    "                            (Electron.convVeto)&\n",
    "                            (Electron.lostHits==0) &\n",
    "                            (Electron.tightCharge==2) &\n",
    "                            (np.abs(Electron.dz) < 0.1) &\n",
    "                           (np.abs(Electron.dxy) < 0.05) &\n",
    "                           (np.abs(Electron.sip3d) < 4) ]\n",
    "        Jet = events.Jet\n",
    "        Jet = Jet[( Jet.pt >25 ) &  (np.abs(Jet.eta ) < 2.4) ]\n",
    "\n",
    "        \n",
    "        def delta_phi(first, second):\n",
    "            return np.arccos(np.cos(first.phi - second.phi))\n",
    "\n",
    "        def delta_r2(first, second):\n",
    "            return (first.eta - second.eta) ** 2 + delta_phi(first, second) ** 2\n",
    "\n",
    "        def match(first, second, deltaRCut=0.4):\n",
    "            drCut2 = deltaRCut**2\n",
    "            combs = ak.cartesian([first, second], nested=True)\n",
    "            return ak.any((delta_r2(combs['0'], combs['1'])<drCut2), axis=2)    \n",
    "        \n",
    "        def mt(pt1, phi1, pt2, phi2):    #transverse mass\n",
    "            return np.sqrt( 2*pt1*pt2 * (1 - np.cos(phi1-phi2)) )\n",
    "        \n",
    "        # Definition of Tight/Loose Muon and Electron\n",
    "        Tight_Muon = Muon[( Muon.pt >20 ) &\n",
    "                          (np.abs(Muon.eta) < 2.4) &\n",
    "                         (Muon.tightId==1) &\n",
    "                          (Muon.miniPFRelIso_all<0.16)\n",
    "                         ]\n",
    "        Tight_Electron = Electron[( Electron.pt >20 )&\n",
    "                          (np.abs(Electron.eta) < 2.4) &\n",
    "                       #  (Electron.tightId==1) &\n",
    "                          (Electron.miniPFRelIso_all<0.12)\n",
    "                         ]\n",
    "        \n",
    "        Loose_Muon = Muon[( Muon.pt >20 )&\n",
    "                          (np.abs(Muon.eta) < 2.4) &\n",
    "                         (Muon.looseId==1) &\n",
    "                          (Muon.miniPFRelIso_all<0.4)\n",
    "                         ]        \n",
    "        \n",
    "        Loose_Electron = Electron[( Electron.pt >20 )&\n",
    "                          (np.abs(Electron.eta) < 2.4) &\n",
    "                        # (Electron.looseId==1) &\n",
    "                          (Electron.miniPFRelIso_all<0.4)\n",
    "                         ]\n",
    "        MET = events.MET   \n",
    "        \n",
    "\n",
    "        control_region_Muon = events[( ak.num(Loose_Muon)==1) &(ak.num(Loose_Electron)==0)\n",
    "                                    & (ak.num(Jet[~match(Jet, Muon, deltaRCut=1.0)])>=1) & ( MET.pt<20) \n",
    "                                    & ak.any(mt(Muon.pt,Muon.phi, MET.pt, MET.phi)<20, axis=1)]\n",
    "\n",
    "        \n",
    "        tight_muon= control_region_Muon.Muon[(control_region_Muon.Muon.pt >20 ) &\n",
    "                          (np.abs(control_region_Muon.Muon.eta) < 2.4) &\n",
    "                         (control_region_Muon.Muon.tightId==1) &\n",
    "                          (control_region_Muon.Muon.miniPFRelIso_all<0.16)\n",
    "                         ]\n",
    "        output['Tight_Muon'].fill(dataset=dataset, pt=ak.flatten(tight_muon.pt), eta=ak.flatten(tight_muon.eta))\n",
    "        output['Muon'].fill(dataset=dataset, pt=ak.flatten(control_region_Muon.Muon.pt), eta=ak.flatten(control_region_Muon.Muon.eta))\n",
    "        return output\n",
    "    def postprocess(self, accumulator):\n",
    "        return accumulator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The folder QCD containes a file `Samples.txt` with all desired samples, and a script `query.sh`. With the command `./query.sh` (with executing permission), one can generate text files containing all the root file names corresponding to the samples in `Samples.txt` using [dasgoclient](https://github.com/dmwm/dasgoclient).\n",
    "\n",
    "Note that you need your grid passphrase for setting up the proxy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Samples = glob.glob(\"QCD/QCD*.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileset = {}\n",
    "for n in Samples:\n",
    "    name = n.split(\"/\")[1].strip(\".txt\")\n",
    "    file = open(n,mode='r')\n",
    "    content = file.read()\n",
    "    file.close()\n",
    "    lst = content.split(\"\\n\")\n",
    "    if lst[-1]=='':\n",
    "        lst = lst[:-1]\n",
    "    lst = [\"root://xcache/\" + s for s in lst]  \n",
    "    fileset[name]=lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'protocol': 'tls://', 'security': Security(require_encryption=True, tls_ca_file='/etc/cmsaf-secrets/ca.pem', tls_client_cert='/etc/cmsaf-secrets/hostcert.pem', tls_client_key='/etc/cmsaf-secrets/hostcert.pem', tls_scheduler_cert='/etc/cmsaf-secrets/hostcert.pem', tls_scheduler_key='/etc/cmsaf-secrets/hostcert.pem', tls_worker_cert='/etc/cmsaf-secrets/hostcert.pem', tls_worker_key='/etc/cmsaf-secrets/hostcert.pem'), 'log_directory': 'logs', 'silence_logs': 'DEBUG', 'scheduler_options': {'port': 8786, 'dashboard_address': '8787', 'protocol': 'tls', 'external_address': 'tls://matousadamec-40gmail-2ecom.dask.coffea.casa:8786'}, 'job_extra': {'universe': 'docker', 'docker_image': 'coffeateam/coffea-casa-analysis:2021.05.06post0', 'container_service_names': 'dask', 'dask_container_port': 8786, 'transfer_input_files': '/etc/cmsaf-secrets/ca.pem, /etc/cmsaf-secrets/hostcert.pem, /etc/cmsaf-secrets/xcache_token', 'encrypt_input_files': '/etc/cmsaf-secrets/ca.pem, /etc/cmsaf-secrets/hostcert.pem, /etc/cmsaf-secrets/xcache_token', 'transfer_output_files': '', 'when_to_transfer_output': 'ON_EXIT', 'should_transfer_files': 'YES', 'Stream_Output': 'False', 'Stream_Error': 'False', '+DaskSchedulerAddress': '\"tls://matousadamec-40gmail-2ecom.dask.coffea.casa:8786\"'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.scheduler - INFO - Clear task state\n",
      "distributed.scheduler - INFO -   Scheduler at: tls://192.168.147.176:8786\n",
      "distributed.scheduler - INFO -   dashboard at:                     :8787\n",
      "distributed.core - INFO - Event loop was unresponsive in Scheduler for 9.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "distributed.scheduler - INFO - Register worker <Worker 'tls://matousadamec-40gmail-2ecom.dask-worker.coffea.casa:8788', name: kubernetes-worker-d64ca1ed-6591-482d-b416-84b380efbf5f, memory: 0, processing: 0>\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tls://matousadamec-40gmail-2ecom.dask-worker.coffea.casa:8788\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Receive client connection: Client-e5e692c6-b1d2-11eb-80f3-66450b8b0bb0\n",
      "distributed.core - INFO - Starting established connection\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[                                        ] | 0% Completed |  9.2s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.comm.tcp - INFO - Connection closed before handshake completed\n",
      "distributed.comm.tcp - INFO - Connection closed before handshake completed\n",
      "distributed.comm.tcp - INFO - Connection closed before handshake completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[#                                       ] | 3% Completed | 49.5s"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-02c7c0297fea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dask-report-\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnjobs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"W\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m18\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"F-\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"C.html\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mperformance_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         output = processor.run_uproot_job(files,\n\u001b[0m\u001b[1;32m     56\u001b[0m                                         \u001b[0mtreename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Events'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m                                         \u001b[0mprocessor_instance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mProcessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/coffea/processor/executor.py\u001b[0m in \u001b[0;36mrun_uproot_job\u001b[0;34m(fileset, treename, processor_instance, executor, executor_args, pre_executor, pre_args, chunksize, maxchunks, metadata_cache)\u001b[0m\n\u001b[1;32m   1160\u001b[0m             }\n\u001b[1;32m   1161\u001b[0m             \u001b[0mpre_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpre_arg_override\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1162\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpre_executor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_get\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata_fetcher\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpre_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1163\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1164\u001b[0m                 \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/coffea/processor/executor.py\u001b[0m in \u001b[0;36mdask_executor\u001b[0;34m(items, function, accumulator, **kwargs)\u001b[0m\n\u001b[1;32m    744\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0mdistributed\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprogress\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m             \u001b[0;31m# FIXME: fancy widget doesn't appear, have to live with boring pbar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 746\u001b[0;31m             \u001b[0mprogress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnotebook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    747\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0maccumulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mclevel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0m_decompress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccumulator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/distributed/diagnostics/progressbar.py\u001b[0m in \u001b[0;36mprogress\u001b[0;34m(notebook, multi, complete, *futures, **kwargs)\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m         \u001b[0mTextProgressBar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfutures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomplete\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcomplete\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/distributed/diagnostics/progressbar.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, keys, scheduler, interval, width, loop, complete, start, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0mloop_runner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLoopRunner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0mloop_runner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_sync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlisten\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_draw_bar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremaining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/distributed/utils.py\u001b[0m in \u001b[0;36mrun_sync\u001b[0;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    507\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 509\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0msync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    510\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/distributed/utils.py\u001b[0m in \u001b[0;36msync\u001b[0;34m(loop, func, callback_timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m             \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m         \u001b[0mtyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    556\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 558\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    559\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "from coffea_casa import CoffeaCasaCluster\n",
    "from distributed import Client\n",
    "\n",
    "# Set values at which you wish to run. Each execution takes one index from all the lists. They should thus be of equal size.\n",
    "# multiplies: sets the 'blow-up' factor of the dataset. With 1, run all files. With 2, run all files twice, etc.\n",
    "multiplies = [1]\n",
    "# jobs: sets the number of jobs you're requesting\n",
    "jobs = [50]\n",
    "# chunksizes: sets the various chunk sizes which we run at.\n",
    "chunksizes = [100000]\n",
    "\n",
    "# Initialize dict to keep track of results of each run.\n",
    "results = {'multiply': [], 'workers': [], 'chunksize': [], 'time': [], 'events/s/thread': [], 'events/s': [], 'Dask report': []}\n",
    "\n",
    "# This function ensures that we aren't overwriting Dask report filenames.\n",
    "def unique(filename):\n",
    "    file, ext = os.path.splitext(filename)\n",
    "    counter = 0\n",
    "    while os.path.exists(filename):\n",
    "        counter += 1\n",
    "        filename = file + str(counter) + ext\n",
    "    return filename\n",
    "\n",
    "while (len(multiplies) == len(jobs)) and (len(jobs) == len(chunksizes)) and (len(multiplies) > 0):\n",
    "    # Fileset blow-up is handled here.\n",
    "    multiply = multiplies.pop(0)\n",
    "    results['multiply'].append(multiply)\n",
    "    files = fileset.copy()\n",
    "    keys = fileset.copy().keys()\n",
    "    while multiply > 1:\n",
    "        for key in keys:\n",
    "            files[key+str(multiply)] = files[key]\n",
    "        multiply = multiply - 1\n",
    "    \n",
    "    # Cluster configuration is handled here.\n",
    "    cluster = CoffeaCasaCluster()\n",
    "    njobs = jobs.pop(0)\n",
    "    results['workers'].append(njobs)\n",
    "    cluster.scale(jobs=njobs)\n",
    "    client = Client(cluster)\n",
    "    chunksize = chunksizes.pop(0)\n",
    "    results['chunksize'].append(chunksize)\n",
    "\n",
    "    exe_args = {\n",
    "            'client': client,\n",
    "            'schema': processor.NanoAODSchema,\n",
    "            'savemetrics': True\n",
    "        }\n",
    "\n",
    "    tic = time.time()\n",
    "    from dask.distributed import performance_report\n",
    "    fname = unique(\"dask-report-\" + str(njobs) + \"W\" + str(int(len(files)/18)) + \"F-\" + str(chunksize) + \"C.html\")\n",
    "    with performance_report(filename=fname):\n",
    "        output = processor.run_uproot_job(files,\n",
    "                                        treename = 'Events',\n",
    "                                        processor_instance = Processor(),\n",
    "                                        executor = processor.dask_executor,\n",
    "                                        executor_args = exe_args,\n",
    "                                        chunksize=chunksize\n",
    "                                        )\n",
    "    toc = time.time()\n",
    "\n",
    "    results['time'].append(toc - tic)\n",
    "    results['events/s/thread'].append(output[1]['entries'] / output[1]['processtime'])\n",
    "    results['events/s'].append(output[1]['entries'] / (toc - tic))\n",
    "    results['Dask report'].append(fname)\n",
    "\n",
    "    cluster.close()\n",
    "    # Sleep so cluster can shutdown. 60 seconds is safe; we can probably reduce this, but also, analysis run time is going to be longer... so is 60 seconds really that much of a problem?\n",
    "    if len(jobs) > 1:\n",
    "        time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(results['time'])):\n",
    "    print('Run ' + str(i+1))\n",
    "    for key in results.keys():\n",
    "        if isinstance(results[key][i], int):\n",
    "            print(str(key) + ': ' + str(round(results[key][i], 3)))\n",
    "        else:\n",
    "            print(str(key) + ': ' + str(results[key][i]))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting\n",
    "https://github.com/cmstas/FTAnalysis/blob/run2/analysis/fakes/derivation/ScanChain_fast.C#L161"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "path = 'histos/QCD.pkl.gz'\n",
    "print('Opening path: ', path)\n",
    "hists = {}\n",
    "with gzip.open(path) as fin:\n",
    "    QCD= pickle.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QCD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_mat(Dict):\n",
    "    DICT = {}\n",
    "    mat = np.zeros((7,3))\n",
    "    for key, value in Dict.items():\n",
    "        mat = np.add(mat, value)\n",
    "    return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Loose_Muon = Hist2D.from_bincounts(\n",
    "    combine_mat(QCD[0]['Muon'].sum().values()).T,\n",
    "    (QCD[0]['Muon'].axis('pt').edges(), QCD[0]['Muon'].axis('eta').edges())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Loose_Muon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tight_Muon = Hist2D.from_bincounts(\n",
    "    combine_mat(QCD[0]['Tight_Muon'].sum().values()).T,\n",
    "    (QCD[0]['Tight_Muon'].axis('pt').edges(), QCD[0]['Tight_Muon'].axis('eta').edges())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tight_Muon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tight_Muon.divide(Loose_Muon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
